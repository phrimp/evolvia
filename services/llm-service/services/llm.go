package services

import (
	"bufio"
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"strings"
	"time"

	"llm-service/configs"
	"llm-service/models"
)

type LLMService struct {
	Client   *http.Client
	BaseURL  string
	APIKey   string
	Model    string
	Provider string
}

var llmService *LLMService

func InitLLMService() error {
	llmService = &LLMService{
		Client: &http.Client{
			Timeout: 120 * time.Second, // Increased timeout for LLM responses
		},
		BaseURL:  configs.AppConfig.LLMBaseURL,
		APIKey:   configs.AppConfig.LLMAPIKey,
		Model:    configs.AppConfig.LLMModel,
		Provider: configs.AppConfig.LLMProvider,
	}
	return nil
}

func GetLLMService() *LLMService {
	return llmService
}

func (l *LLMService) IsConnected() bool {
	switch strings.ToLower(l.Provider) {
	case "google", "gemini":
		// For Google Gemini, we can try to make a simple request to check connectivity
		// Since we can't easily test without making an actual API call,
		// we'll just check if the base URL is reachable
		if l.APIKey == "" || l.APIKey == "none" {
			return false
		}
		// For Google API, if we have a valid API key, assume it's connected
		// You could also make a simple test request here if needed
		return true
	case "ollama":
		// Test connection by making a simple request to /models endpoint
		resp, err := http.Get(l.BaseURL + "/models")
		if err != nil {
			return false
		}
		defer resp.Body.Close()
		return resp.StatusCode == 200
	case "openai", "openrouter":
		// For OpenAI-compatible APIs, check if we have an API key
		return l.APIKey != "" && l.APIKey != "none"
	default:
		// For unknown providers, try the models endpoint
		resp, err := http.Get(l.BaseURL + "/models")
		if err != nil {
			return false
		}
		defer resp.Body.Close()
		return resp.StatusCode == 200
	}
}

type ChatCompletionRequest struct {
	Model       string                  `json:"model"`
	Messages    []ChatCompletionMessage `json:"messages"`
	Stream      bool                    `json:"stream"`
	Temperature *float64                `json:"temperature,omitempty"`
	MaxTokens   *int                    `json:"max_tokens,omitempty"`
}

type ChatCompletionMessage struct {
	Role    string `json:"role"`
	Content string `json:"content"`
}

type ChatCompletionResponse struct {
	Choices []struct {
		Message ChatCompletionMessage `json:"message"`
	} `json:"choices"`
}

func (l *LLMService) ProcessChat(userMessage string, userID string) (*models.LLMResponse, error) {
	// Debug logging
	log.Printf("[DEBUG] ProcessChat - UserID: %s, Message: %s", userID, userMessage)

	// Get RAG service
	rag := GetRAGService()
	if rag == nil {
		return nil, fmt.Errorf("RAG service not available")
	}

	// Load prompts from RAG files
	systemPrompt := rag.GetSystemPrompt()
	guardPrompt := rag.GetGuardPrompt()
	ragContext := rag.BuildRAGContext(userID, userMessage)

	// Debug RAG context
	log.Printf("[DEBUG] RAG Context length: %d chars", len(ragContext))
	log.Printf("[DEBUG] RAG Context preview: %.300s...", ragContext)

	// Combine prompts
	fullSystemPrompt := fmt.Sprintf("%s\n\n%s\n\n%s", guardPrompt, systemPrompt, ragContext)

	// Let LLM decide based on guard prompt - don't pre-filter with keywords
	// Try to send request to LLM
	log.Printf("[DEBUG] Sending LLM request for user: %s", userID)
	response, err := l.sendLLMRequest(userMessage, fullSystemPrompt)
	if err != nil {
		log.Printf("[DEBUG] LLM service failed: %v", err)
		log.Printf("[DEBUG] Falling back to intelligent response")
		// Return intelligent fallback response based on RAG context and user query
		return l.generateIntelligentFallbackResponse(userMessage, ragContext, userID), nil
	}

	log.Printf("[DEBUG] LLM response received successfully")

	// Process response
	llmResponse := &models.LLMResponse{
		Message:   response.Choices[0].Message.Content,
		Timestamp: time.Now(),
	}

	return llmResponse, nil
}

func (l *LLMService) sendLLMRequest(userMessage, systemPrompt string) (*ChatCompletionResponse, error) {
	// Prepare chat request
	messages := []ChatCompletionMessage{
		{
			Role:    "system",
			Content: systemPrompt,
		},
		{
			Role:    "user",
			Content: userMessage,
		},
	}

	request := ChatCompletionRequest{
		Model:    l.Model,
		Messages: messages,
		Stream:   false,
	}

	// Send request to LLM
	return l.sendChatRequest(request)
}

// generateIntelligentFallbackResponse provides more intelligent responses based on user query and available context
func (l *LLMService) generateIntelligentFallbackResponse(userMessage, ragContext, userID string) *models.LLMResponse {
	lowerMessage := strings.ToLower(userMessage)

	log.Printf("[DEBUG] Generating intelligent fallback for: %s", userMessage)
	log.Printf("[DEBUG] RAG Context available: %v", strings.Contains(ragContext, "TH√îNG TIN NG∆Ø·ªúI D√ôNG"))

	// Check if we have user data in RAG context
	hasUserData := strings.Contains(ragContext, "TH√îNG TIN NG∆Ø·ªúI D√ôNG") ||
		strings.Contains(ragContext, "user_id") ||
		strings.Contains(ragContext, "userId")

	if hasUserData {
		// User is logged in and we have their data
		log.Printf("[DEBUG] User data found, processing specific query")

		// Handle name queries specifically
		if strings.Contains(lowerMessage, "t√™n") || strings.Contains(lowerMessage, "t√¥i l√† ai") {
			nameInfo := l.extractNameFromContext(ragContext)
			if nameInfo != "" {
				return &models.LLMResponse{
					Message:   nameInfo,
					Timestamp: time.Now(),
				}
			}

			// If no name found in context, be more helpful
			return &models.LLMResponse{
				Message: `T√¥i ƒë√£ t√¨m th·∫•y th√¥ng tin t√†i kho·∫£n c·ªßa b·∫°n trong h·ªá th·ªëng, nh∆∞ng ch∆∞a th·ªÉ tr√≠ch xu·∫•t t√™n c·ª• th·ªÉ. 

C√≥ th·ªÉ th√¥ng tin t√™n ch∆∞a ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·∫ßy ƒë·ªß. B·∫°n c√≥ mu·ªën:
‚Ä¢ C·∫≠p nh·∫≠t th√¥ng tin t√™n trong h·ªì s∆°
‚Ä¢ Xem th√¥ng tin t√†i kho·∫£n hi·ªán t·∫°i
‚Ä¢ Li√™n h·ªá h·ªó tr·ª£ ƒë·ªÉ c·∫≠p nh·∫≠t th√¥ng tin

T√¥i c√≥ th·ªÉ gi√∫p b·∫°n th√™m g√¨ kh√°c kh√¥ng?`,
				Timestamp: time.Now(),
			}
		}

		// Handle email queries
		if strings.Contains(lowerMessage, "email") {
			emailInfo := l.extractEmailFromContext(ragContext)
			if emailInfo != "" {
				return &models.LLMResponse{
					Message:   emailInfo,
					Timestamp: time.Now(),
				}
			}
		}

		// Handle general account info queries
		if strings.Contains(lowerMessage, "t√†i kho·∫£n") || strings.Contains(lowerMessage, "th√¥ng tin") {
			return &models.LLMResponse{
				Message: fmt.Sprintf(`T√¥i ƒë√£ t√¨m th·∫•y th√¥ng tin t√†i kho·∫£n c·ªßa b·∫°n trong h·ªá th·ªëng:

%s

B·∫°n c√≥ mu·ªën bi·∫øt th√™m chi ti·∫øt v·ªÅ ph·∫ßn n√†o kh√¥ng? T√¥i c√≥ th·ªÉ gi√∫p b·∫°n v·ªõi:
‚Ä¢ Th√¥ng tin c√° nh√¢n (t√™n, email, s·ªë ƒëi·ªán tho·∫°i)
‚Ä¢ L·ªãch s·ª≠ giao d·ªãch v√† thanh to√°n
‚Ä¢ C√†i ƒë·∫∑t t√†i kho·∫£n
‚Ä¢ B·∫£o m·∫≠t t√†i kho·∫£n

Vui l√≤ng cho t√¥i bi·∫øt b·∫°n c·∫ßn h·ªó tr·ª£ g√¨!`, ragContext),
				Timestamp: time.Now(),
			}
		}

		// For other queries when user is logged in
		return &models.LLMResponse{
			Message: `Xin ch√†o! T√¥i ƒë√£ nh·∫≠n d·∫°ng ƒë∆∞·ª£c t√†i kho·∫£n c·ªßa b·∫°n trong h·ªá th·ªëng.

T√¥i c√≥ th·ªÉ gi√∫p b·∫°n v·ªõi:
‚Ä¢ Th√¥ng tin t√†i kho·∫£n v√† h·ªì s∆° c√° nh√¢n
‚Ä¢ L·ªãch s·ª≠ giao d·ªãch v√† ƒë∆°n h√†ng
‚Ä¢ C√†i ƒë·∫∑t v√† b·∫£o m·∫≠t t√†i kho·∫£n
‚Ä¢ H·ªó tr·ª£ k·ªπ thu·∫≠t
‚Ä¢ H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng d·ªãch v·ª•

B·∫°n mu·ªën bi·∫øt th√¥ng tin g√¨ c·ª• th·ªÉ?`,
			Timestamp: time.Now(),
		}
	}

	// User not logged in or no user data available
	if strings.Contains(lowerMessage, "t√™n") || strings.Contains(lowerMessage, "t√¥i l√† ai") {
		return &models.LLMResponse{
			Message: `ƒê·ªÉ bi·∫øt t√™n c·ªßa b·∫°n, t√¥i c·∫ßn truy c·∫≠p th√¥ng tin t√†i kho·∫£n c·ªßa b·∫°n. 

üîê **Vui l√≤ng ƒëƒÉng nh·∫≠p ƒë·ªÉ t√¥i c√≥ th·ªÉ:**
‚Ä¢ Truy xu·∫•t th√¥ng tin t√™n ƒë·∫ßy ƒë·ªß
‚Ä¢ Hi·ªÉn th·ªã th√¥ng tin t√†i kho·∫£n an to√†n
‚Ä¢ Cung c·∫•p h·ªó tr·ª£ c√° nh√¢n h√≥a

Sau khi ƒëƒÉng nh·∫≠p, ch·ªâ c·∫ßn h·ªèi l·∫°i "T√™n t√¥i l√† g√¨?" v√† t√¥i s·∫Ω tr·∫£ l·ªùi ngay!

üí° **Hi·ªán t·∫°i t√¥i c√≥ th·ªÉ gi√∫p b·∫°n:**
‚Ä¢ H∆∞·ªõng d·∫´n ƒëƒÉng nh·∫≠p
‚Ä¢ Th√¥ng tin d·ªãch v·ª• Evolvia
‚Ä¢ H·ªó tr·ª£ k·ªπ thu·∫≠t

B·∫°n c·∫ßn h·ªó tr·ª£ g√¨?`,
			Timestamp: time.Now(),
		}
	}

	if strings.Contains(lowerMessage, "t√†i kho·∫£n") || strings.Contains(lowerMessage, "th√¥ng tin") {
		return &models.LLMResponse{
			Message: `ƒê·ªÉ truy c·∫≠p th√¥ng tin t√†i kho·∫£n, b·∫°n c·∫ßn ƒëƒÉng nh·∫≠p tr∆∞·ªõc. 

üîê **Vui l√≤ng:**
1. ƒêƒÉng nh·∫≠p v·ªõi t√†i kho·∫£n c·ªßa b·∫°n
2. Cung c·∫•p JWT token h·ª£p l·ªá trong header Authorization

Sau khi ƒëƒÉng nh·∫≠p, t√¥i s·∫Ω c√≥ th·ªÉ truy xu·∫•t v√† hi·ªÉn th·ªã ƒë·∫ßy ƒë·ªß th√¥ng tin t√†i kho·∫£n c·ªßa b·∫°n m·ªôt c√°ch an to√†n.

üí° **T√¥i c√≥ th·ªÉ gi√∫p b·∫°n v·ªõi:**
‚Ä¢ H∆∞·ªõng d·∫´n ƒëƒÉng nh·∫≠p
‚Ä¢ H·ªó tr·ª£ k·ªπ thu·∫≠t
‚Ä¢ Th√¥ng tin d·ªãch v·ª•

B·∫°n c·∫ßn h·ªó tr·ª£ g√¨?`,
			Timestamp: time.Now(),
		}
	}

	// Default response for other queries
	return &models.LLMResponse{
		Message: `Ch√†o b·∫°n! T√¥i l√† tr·ª£ l√Ω AI c·ªßa Evolvia. 

T√¥i c√≥ th·ªÉ gi√∫p b·∫°n v·ªõi:
‚Ä¢ Th√¥ng tin t√†i kho·∫£n (c·∫ßn ƒëƒÉng nh·∫≠p)
‚Ä¢ H·ªó tr·ª£ k·ªπ thu·∫≠t
‚Ä¢ H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng d·ªãch v·ª•
‚Ä¢ Th√¥ng tin v·ªÅ c√°c t√≠nh nƒÉng Evolvia

B·∫°n mu·ªën h·ªó tr·ª£ g√¨ c·ª• th·ªÉ?`,
		Timestamp: time.Now(),
	}
}

func (l *LLMService) sendChatRequest(request ChatCompletionRequest) (*ChatCompletionResponse, error) {
	// Handle different providers
	switch strings.ToLower(l.Provider) {
	case "google", "gemini":
		return l.sendGoogleGeminiRequest(request)
	default:
		return l.sendOpenAICompatibleRequest(request)
	}
}

func (l *LLMService) sendGoogleGeminiRequest(request ChatCompletionRequest) (*ChatCompletionResponse, error) {
	// Convert OpenAI format to Google Gemini format
	var contents []map[string]interface{}

	for _, msg := range request.Messages {
		if msg.Role == "system" {
			// For system messages, we'll prepend them to the user message
			// Google Gemini doesn't have a separate system role
			continue
		}

		content := map[string]interface{}{
			"parts": []map[string]interface{}{
				{"text": msg.Content},
			},
		}
		contents = append(contents, content)
	}

	// If we have a system message, prepend it to the first user message
	var systemPrompt string
	for _, msg := range request.Messages {
		if msg.Role == "system" {
			systemPrompt = msg.Content
			break
		}
	}

	// Combine system prompt with user message if exists
	if systemPrompt != "" && len(contents) > 0 {
		if parts, ok := contents[0]["parts"].([]map[string]interface{}); ok && len(parts) > 0 {
			if text, ok := parts[0]["text"].(string); ok {
				parts[0]["text"] = systemPrompt + "\n\nUser: " + text
			}
		}
	}

	geminiRequest := map[string]interface{}{
		"contents": contents,
	}

	jsonData, err := json.Marshal(geminiRequest)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %v", err)
	}

	// Use the base URL as-is since it already includes the generateContent endpoint
	req, err := http.NewRequest("POST", l.BaseURL+"?key="+l.APIKey, bytes.NewBuffer(jsonData))
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %v", err)
	}

	req.Header.Set("Content-Type", "application/json")

	log.Printf("Sending request to Google Gemini API...")
	resp, err := l.Client.Do(req)
	if err != nil {
		return nil, fmt.Errorf("request failed: %v", err)
	}
	defer resp.Body.Close()

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("failed to read response: %v", err)
	}

	log.Printf("Received response with status: %d", resp.StatusCode)
	if resp.StatusCode != 200 {
		return nil, fmt.Errorf("google Gemini API error (status %d): %s", resp.StatusCode, string(body))
	}

	// Parse Google Gemini response format
	var geminiResponse struct {
		Candidates []struct {
			Content struct {
				Parts []struct {
					Text string `json:"text"`
				} `json:"parts"`
			} `json:"content"`
		} `json:"candidates"`
	}

	err = json.Unmarshal(body, &geminiResponse)
	if err != nil {
		return nil, fmt.Errorf("failed to parse Gemini response: %v", err)
	}

	// Convert back to OpenAI format for compatibility
	response := &ChatCompletionResponse{
		Choices: []struct {
			Message ChatCompletionMessage `json:"message"`
		}{
			{
				Message: ChatCompletionMessage{
					Role:    "assistant",
					Content: "",
				},
			},
		},
	}

	if len(geminiResponse.Candidates) > 0 && len(geminiResponse.Candidates[0].Content.Parts) > 0 {
		response.Choices[0].Message.Content = geminiResponse.Candidates[0].Content.Parts[0].Text
	}

	return response, nil
}

func (l *LLMService) sendOpenAICompatibleRequest(request ChatCompletionRequest) (*ChatCompletionResponse, error) {
	jsonData, err := json.Marshal(request)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %v", err)
	}

	log.Printf("Making request to: %s", l.BaseURL+"/chat/completions")
	req, err := http.NewRequest("POST", l.BaseURL+"/chat/completions", bytes.NewBuffer(jsonData))
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %v", err)
	}

	req.Header.Set("Content-Type", "application/json")
	if l.APIKey != "" && l.APIKey != "none" {
		req.Header.Set("Authorization", "Bearer "+l.APIKey)
	}

	log.Printf("Sending request to LLM API...")
	resp, err := l.Client.Do(req)
	if err != nil {
		return nil, fmt.Errorf("request failed: %v", err)
	}
	defer resp.Body.Close()

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("failed to read response: %v", err)
	}

	log.Printf("Received response with status: %d", resp.StatusCode)
	if resp.StatusCode != 200 {
		return nil, fmt.Errorf("LLM API error (status %d): %s", resp.StatusCode, string(body))
	}

	var response ChatCompletionResponse
	err = json.Unmarshal(body, &response)
	if err != nil {
		return nil, err
	}
	return &response, nil
}

// ProcessChatStream processes chat message and returns streaming response
func (l *LLMService) ProcessChatStream(userMessage string, userID string, responseChan chan models.StreamChunk) {
	defer close(responseChan)

	// Get RAG service
	rag := GetRAGService()
	if rag == nil {
		responseChan <- models.StreamChunk{
			Content:   "",
			IsEnd:     true,
			Error:     "RAG service not available",
			Timestamp: time.Now(),
		}
		return
	}

	// Load prompts from RAG files
	systemPrompt := rag.GetSystemPrompt()
	guardPrompt := rag.GetGuardPrompt()
	ragContext := rag.BuildRAGContext(userID, userMessage)

	// Combine prompts
	fullSystemPrompt := fmt.Sprintf("%s\n\n%s\n\n%s", guardPrompt, systemPrompt, ragContext)

	log.Printf("[DEBUG] Sending streaming LLM request for user: %s", userID)

	// Try to send streaming request to LLM
	err := l.sendStreamingLLMRequest(userMessage, fullSystemPrompt, responseChan)
	if err != nil {
		log.Printf("[DEBUG] LLM streaming service failed: %v", err)
		log.Printf("[DEBUG] Falling back to non-streaming response")

		// Try to get a regular response first
		response, regularErr := l.sendLLMRequest(userMessage, fullSystemPrompt)
		if regularErr != nil {
			log.Printf("[DEBUG] Regular LLM request also failed: %v", regularErr)
			// Generate intelligent fallback response based on RAG context and user query
			fallbackResponse := l.generateIntelligentFallbackResponse(userMessage, ragContext, userID)
			l.simulateStreaming(fallbackResponse.Message, responseChan)
		} else {
			// Use the successful regular response and simulate streaming
			log.Printf("[DEBUG] Regular LLM request succeeded, simulating streaming")
			l.simulateStreaming(response.Choices[0].Message.Content, responseChan)
		}
	}
}

func (l *LLMService) sendStreamingLLMRequest(userMessage, systemPrompt string, responseChan chan models.StreamChunk) error {
	// Check provider first - some providers don't support streaming
	switch strings.ToLower(l.Provider) {
	case "google", "gemini":
		// Google Gemini doesn't support streaming in the same way
		// Fall back to regular request and simulate streaming
		log.Printf("[DEBUG] Google Gemini provider - using simulated streaming")
		return fmt.Errorf("provider %s doesn't support true streaming", l.Provider)
	case "ollama":
		// Ollama supports streaming
		return l.sendOpenAIStreamingRequest(userMessage, systemPrompt, responseChan)
	case "openai", "openrouter":
		// OpenAI and OpenRouter support streaming
		return l.sendOpenAIStreamingRequest(userMessage, systemPrompt, responseChan)
	default:
		// Try OpenAI-compatible streaming for unknown providers
		log.Printf("[DEBUG] Unknown provider %s - trying OpenAI-compatible streaming", l.Provider)
		return l.sendOpenAIStreamingRequest(userMessage, systemPrompt, responseChan)
	}
}

func (l *LLMService) sendOpenAIStreamingRequest(userMessage, systemPrompt string, responseChan chan models.StreamChunk) error {
	// Prepare chat request with streaming enabled
	messages := []ChatCompletionMessage{
		{
			Role:    "system",
			Content: systemPrompt,
		},
		{
			Role:    "user",
			Content: userMessage,
		},
	}

	request := ChatCompletionRequest{
		Model:    l.Model,
		Messages: messages,
		Stream:   true, // Enable streaming
	}

	jsonData, err := json.Marshal(request)
	if err != nil {
		return fmt.Errorf("failed to marshal request: %v", err)
	}

	log.Printf("[DEBUG] Making streaming request to: %s", l.BaseURL+"/chat/completions")
	req, err := http.NewRequest("POST", l.BaseURL+"/chat/completions", bytes.NewBuffer(jsonData))
	if err != nil {
		return fmt.Errorf("failed to create request: %v", err)
	}

	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("Accept", "text/event-stream")
	req.Header.Set("Cache-Control", "no-cache")
	if l.APIKey != "" && l.APIKey != "none" {
		req.Header.Set("Authorization", "Bearer "+l.APIKey)
	}

	resp, err := l.Client.Do(req)
	if err != nil {
		return fmt.Errorf("streaming request failed: %v", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != 200 {
		body, _ := io.ReadAll(resp.Body)
		return fmt.Errorf("LLM API error (status %d): %s", resp.StatusCode, string(body))
	}

	log.Printf("[DEBUG] Started receiving streaming response...")

	// Use a scanner to read line by line more reliably
	scanner := bufio.NewScanner(resp.Body)

	for scanner.Scan() {
		line := strings.TrimSpace(scanner.Text())

		if line == "" {
			continue
		}

		if line == "data: [DONE]" {
			responseChan <- models.StreamChunk{
				Content:   "",
				IsEnd:     true,
				Timestamp: time.Now(),
			}
			return nil
		}

		if strings.HasPrefix(line, "data: ") {
			data := line[6:] // Remove "data: " prefix
			if data == "[DONE]" {
				responseChan <- models.StreamChunk{
					Content:   "",
					IsEnd:     true,
					Timestamp: time.Now(),
				}
				return nil
			}

			var streamResp models.StreamingResponse
			if err := json.Unmarshal([]byte(data), &streamResp); err != nil {
				log.Printf("[DEBUG] Failed to parse streaming response: %v", err)
				continue
			}

			if len(streamResp.Choices) > 0 {
				content := streamResp.Choices[0].Delta.Content
				if content != "" {
					responseChan <- models.StreamChunk{
						Content:   content,
						IsEnd:     false,
						Timestamp: time.Now(),
					}
				}

				if streamResp.Choices[0].FinishReason != nil {
					responseChan <- models.StreamChunk{
						Content:   "",
						IsEnd:     true,
						Timestamp: time.Now(),
					}
					return nil
				}
			}
		}
	}

	if err := scanner.Err(); err != nil {
		return fmt.Errorf("error reading streaming response: %v", err)
	}

	// If we reach here without seeing [DONE], send end signal
	responseChan <- models.StreamChunk{
		Content:   "",
		IsEnd:     true,
		Timestamp: time.Now(),
	}

	return nil
}

func (l *LLMService) simulateStreaming(message string, responseChan chan models.StreamChunk) {
	words := strings.Fields(message)

	for i, word := range words {
		// Add space before word (except first word)
		content := word
		if i > 0 {
			content = " " + word
		}

		responseChan <- models.StreamChunk{
			Content:   content,
			IsEnd:     false,
			Timestamp: time.Now(),
		}

		// Small delay to simulate typing
		time.Sleep(50 * time.Millisecond)
	}

	// Send end signal
	responseChan <- models.StreamChunk{
		Content:   "",
		IsEnd:     true,
		Timestamp: time.Now(),
	}
}

// extractNameFromContext extracts name information from RAG context
func (l *LLMService) extractNameFromContext(ragContext string) string {
	// Check for various name fields that might exist in the database
	nameFields := []string{"name", "fullName", "full_name", "firstName", "first_name", "lastName", "last_name", "displayName", "display_name"}

	// First try to find structured data (JSON-like format)
	lines := strings.Split(ragContext, "\n")
	for _, line := range lines {
		line = strings.TrimSpace(line)

		// Check for various name formats in the data
		for _, field := range nameFields {
			// Check for key:value format (like "name: John Doe")
			if strings.Contains(strings.ToLower(line), field+":") {
				parts := strings.SplitN(line, ":", 2)
				if len(parts) == 2 {
					name := strings.TrimSpace(parts[1])
					if name != "" && name != "null" && name != "undefined" {
						return fmt.Sprintf("T√™n c·ªßa b·∫°n l√†: %s", name)
					}
				}
			}

			// Check for JSON-like format
			if strings.Contains(strings.ToLower(line), `"`+field+`"`) {
				// Try to extract value after the field name
				fieldPattern := `"` + field + `"`
				if idx := strings.Index(strings.ToLower(line), fieldPattern); idx != -1 {
					remaining := line[idx+len(fieldPattern):]
					if colonIdx := strings.Index(remaining, ":"); colonIdx != -1 {
						valueStart := colonIdx + 1
						value := strings.TrimSpace(remaining[valueStart:])
						// Remove quotes and trailing comma if present
						value = strings.Trim(value, `"`)
						if commaIdx := strings.Index(value, ","); commaIdx != -1 {
							value = value[:commaIdx]
						}
						value = strings.TrimSpace(value)
						if value != "" && value != "null" && value != "undefined" {
							return fmt.Sprintf("T√™n c·ªßa b·∫°n l√†: %s", value)
						}
					}
				}
			}
		}

		// Also check for Vietnamese format
		if strings.Contains(strings.ToLower(line), "t√™n:") {
			parts := strings.SplitN(line, ":", 2)
			if len(parts) == 2 {
				name := strings.TrimSpace(parts[1])
				if name != "" && name != "null" && name != "undefined" {
					return fmt.Sprintf("T√™n c·ªßa b·∫°n l√†: %s", name)
				}
			}
		}
	}

	return ""
}

// extractEmailFromContext extracts email information from RAG context
func (l *LLMService) extractEmailFromContext(ragContext string) string {
	// Check for various email fields that might exist in the database
	emailFields := []string{"email", "emailAddress", "email_address", "userEmail", "user_email", "contactEmail"}

	lines := strings.Split(ragContext, "\n")
	for _, line := range lines {
		line = strings.TrimSpace(line)

		// Check for various email formats in the data
		for _, field := range emailFields {
			// Check for key:value format (like "email: user@example.com")
			if strings.Contains(strings.ToLower(line), field+":") {
				parts := strings.SplitN(line, ":", 2)
				if len(parts) == 2 {
					email := strings.TrimSpace(parts[1])
					if email != "" && email != "null" && email != "undefined" && strings.Contains(email, "@") {
						return fmt.Sprintf("Email c·ªßa b·∫°n l√†: %s", email)
					}
				}
			}

			// Check for JSON-like format
			if strings.Contains(strings.ToLower(line), `"`+field+`"`) {
				fieldPattern := `"` + field + `"`
				if idx := strings.Index(strings.ToLower(line), fieldPattern); idx != -1 {
					remaining := line[idx+len(fieldPattern):]
					if colonIdx := strings.Index(remaining, ":"); colonIdx != -1 {
						valueStart := colonIdx + 1
						value := strings.TrimSpace(remaining[valueStart:])
						value = strings.Trim(value, `"`)
						if commaIdx := strings.Index(value, ","); commaIdx != -1 {
							value = value[:commaIdx]
						}
						value = strings.TrimSpace(value)
						if value != "" && value != "null" && value != "undefined" && strings.Contains(value, "@") {
							return fmt.Sprintf("Email c·ªßa b·∫°n l√†: %s", value)
						}
					}
				}
			}
		}
	}

	return ""
}
